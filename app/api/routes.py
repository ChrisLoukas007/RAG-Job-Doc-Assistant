# HTTP layer only â€” your original endpoints, kept, just moved into a router.
# Kept your comments and SSE behavior.
import json
import time
import re  # for pattern matching in text 
from pathlib import Path  # for handling file paths safely
from typing import Any, List

from fastapi import APIRouter, Depends, HTTPException, Query, Body
from fastapi.responses import StreamingResponse

from ..core.config import LOGS_DIR
from ..core.logging import get_logger
from ..models.schemas import QueryIn, RAGOut, FeedbackIn
from ..deps import get_chain, get_retriever
from ..pipelines.rag import stream_chain_answer

router = APIRouter()
logger = get_logger(__name__)

# Utilities - helper functions  (kept from your main.py)
BULLET_RE = re.compile(r"^\s*[-*]\s+(.*)$")

def coerce_to_text(resp: Any) -> str:
    """Normalize various LangChain/LLM outputs into plain text."""
    try:
        from langchain_core.messages import BaseMessage  # type: ignore
        if isinstance(resp, BaseMessage):
            return str(resp.content or "")
    except Exception:
        pass

    if isinstance(resp, str):
        return resp

    if isinstance(resp, dict):
        if isinstance(resp.get("content"), str):
            return resp["content"]
        if "text" in resp:
            return str(resp["text"])
        if "answer" in resp:
            return str(resp["answer"])
        return str(resp)

    if isinstance(resp, list):
        parts: List[str] = []
        for item in resp:
            if isinstance(item, str):
                parts.append(item)
            elif isinstance(item, dict) and "content" in item:
                parts.append(str(item["content"]))
            else:
                parts.append(str(item))
        return "\n".join(parts)

    return str(resp)

# API ENDPOINTS - The actual web services (kept)
@router.get("/")
def home():
    return {"msg": "RAG Helpdesk API. See /docs or /health."}

@router.get("/health")
def health():
    return {"status": "ok"}

@router.post("/query", response_model=RAGOut)
async def query(
    payload: QueryIn = Body(...),
    chain = Depends(get_chain),
    retriever = Depends(get_retriever),
):
    t0 = time.time()
    try:
        # 1) Ask the chain for an answer
        raw = await chain.ainvoke(payload.question)
        answer_text: str = coerce_to_text(raw)  # Normalize to plain text

        # 2) Try to find source files mentioned in the answer
        parsed: List[str] = []
        for line in answer_text.splitlines():
            m = BULLET_RE.match(line)
            if m:
                parsed.append(m.group(1).strip())

        # 3) If no sources found in answer, get them from the search results
        if not parsed:
            docs = await retriever.aget_relevant_documents(payload.question)
            for d in docs:
                src = d.metadata.get("source", "unknown")
                parsed.append(Path(src).name)

        # 4) Remove duplicates while keeping order
        seen = set()
        sources: List[str] = []
        for s in parsed:
            if s and s not in seen:
                seen.add(s)
                sources.append(Path(s).name)

        return RAGOut(
            answer=answer_text,
            sources=sources,
            latency_ms=(time.time() - t0) * 1000.0,
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/stream")
async def stream(
    q: str = Query(..., description="User question"),
    chain = Depends(get_chain),
    retriever = Depends(get_retriever),
):
    """
    STREAMING ENDPOINT: Get answers word-by-word as they're generated by the model. 
    This uses Server-Sent Events (SSE) to send data in real-time    
    Events sent:
    - 'meta': Status updates and citations
    - 'token': Each word/piece of the answer
    - 'done': Finished
    - 'error': If something went wrong
    """
    async def event_generator():
        """
        This function generates the streaming response
        """
        try:
            # Tell client we started
            yield "event: meta\ndata: " + json.dumps({"status": "started"}) + "\n\n"

            # Send citations early (which documents will be used)
            try:
                docs = await retriever.aget_relevant_documents(q)
                citations = [
                    {
                        "title": Path(d.metadata.get("source", "unknown")).name,
                        "url": d.metadata.get("source", "")
                    }
                    for d in docs
                ]
                yield "event: meta\ndata: " + json.dumps({"citations": citations}) + "\n\n"
            except Exception:
                # Don't break the stream if citations fail
                pass

            # Stream the answer token by token
            async for piece in stream_chain_answer(chain, q):
                # Send each piece of text as it's generated
                yield "event: token\ndata: " + json.dumps({"token": piece}) + "\n\n"

            # Signal that we're done
            yield "event: done\ndata: {}\n\n"
        except Exception as e:
            # If error occurs, tell client and close gracefully
            yield "event: error\ndata: " + json.dumps({"message": str(e)}) + "\n\n"
            yield "event: done\ndata: {}\n\n"

    headers = {
        "Cache-Control": "no-cache",      # Don't cache the stream
        "Connection": "keep-alive",       # Keep connection open
        "X-Accel-Buffering": "no",        # Tell nginx not to buffer
    }
    return StreamingResponse(event_generator(), media_type="text/event-stream", headers=headers)

# Feedback endpoint to collect user ratings/comments (kept)
@router.post("/feedback")
def feedback(fb: FeedbackIn):
    rec = fb.model_dump() | {"ts": time.time()}
    Path(LOGS_DIR).mkdir(parents=True, exist_ok=True)
    with open(Path(LOGS_DIR) / "feedback.jsonl", "a", encoding="utf-8") as f:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    return {"status": "logged"}
