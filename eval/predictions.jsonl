{"question": "What is RAG in one sentence?", "correct_answer": "RAG retrieves relevant chunks from your documents and asks an LLM to answer using only those chunks, with citations.", "ai_answer": "RAG (Relevance Aware Generator) fetches relevant text chunks from a private corpus and asks an LLM to answer using only those chunks, with citations.\n\nSources:\n• data/raw/RAG_basics.md\n• data/raw/LangChain_note.md", "similarity_score": 0.7968491315841675, "response_time_ms": 4305.1}
{"question": "Where are FastAPI interactive docs?", "correct_answer": "At the /docs route, which serves a Swagger UI for trying endpoints.", "ai_answer": "FastAPI interactive docs are at `/docs` via Swagger.\n\nSources:\n• data/raw/FastAPI_cheatsheet.md\n• data/raw/LangChain_note.md\n• data/raw/RAG_basics.md\n• data/raw/Docker_quickstart.txt", "similarity_score": 0.6141709685325623, "response_time_ms": 3966.6}
{"question": "What does the /upload endpoint do?", "correct_answer": "It accepts a PDF file, extracts its text, splits it into chunks, and stores them in a vector database.", "ai_answer": "The `/upload` endpoint is not mentioned in the provided context.\n\nSources:\n• Docker_quickstart.txt\n• LangChain_note.md\n• RAG_basics.md\n• FastAPI_cheatsheet.md", "similarity_score": 0.05278211086988449, "response_time_ms": 3493.1}
{"question": "What is the purpose of the /ask endpoint?", "correct_answer": "It takes a user question, retrieves relevant document chunks from the vector database, and generates an answer using an LLM.", "ai_answer": "I don't know.\n\nSources:\n• LangChain_note.md\n• RAG_basics.md\n• FastAPI_cheatsheet.md\n• Docker_quickstart.txt", "similarity_score": 0.16616442799568176, "response_time_ms": 3092.8}
{"question": "How are documents processed before being stored?", "correct_answer": "Text is extracted from PDFs, split into chunks with overlap, and then embedded into vectors for storage in a vector database.", "ai_answer": "The documents are processed before being stored by using volumes to persist indexes/logs in a running container.\n\nSources:\n• Docker_quickstart.txt\n• LangChain_note.md\n• RAG_basics.md\n• FastAPI_cheatsheet.md", "similarity_score": 0.2583414614200592, "response_time_ms": 3750.6}
{"question": "What libraries are used for PDF text extraction and chunking?", "correct_answer": "PyMuPDF (fitz) is used for PDF text extraction, and LangChain's RecursiveCharacterTextSplitter is used for chunking.", "ai_answer": "I don't know.\n\nSources:\n• data/raw/RAG_basics.md\n• data/raw/LangChain_note.md\n• data/raw/Docker_quickstart.txt\n• data/raw/FastAPI_cheatsheet.md", "similarity_score": 0.16849443316459656, "response_time_ms": 3686.2}
{"question": "What environment variables are required for this application?", "correct_answer": "OPENAI_API_KEY for OpenAI access and PINECONE_API_KEY and PINECONE_ENVIRONMENT for Pinecone vector database access.", "ai_answer": "The environment variables required for this application are:\n\n`PYTHONPATH`, `LD_LIBRARY_PATH`\n\n* data/raw/LangChain_note.md\n* data/raw/Docker_quickstart.txt\n* data/raw/FastAPI_cheatsheet.md\n* data/raw/RAG_basics.md", "similarity_score": 0.22157540917396545, "response_time_ms": 4109.8}
{"question": "How does the application ensure that answers are based on document content?", "correct_answer": "By retrieving relevant chunks from the vector database and using them as context for the LLM to generate answers.", "ai_answer": "The application ensures that answers are based on document content by retrieving relevant text chunks from a private corpus using FAISS (or hosted alternatives Qdrant/Pinecone) and asking an LLM to answer using only those chunks, with citations.\n\nSources:\n• data/raw/RAG_basics.md\n• data/raw/LangChain_note.md", "similarity_score": 0.5367432236671448, "response_time_ms": 4751.0}
{"question": "What happens if no relevant documents are found for a question?", "correct_answer": "The application raises an HTTP 404 error indicating no relevant documents were found.", "ai_answer": "If no relevant documents are found for a question, the RAG (Relevance Aware Generator) will not be able to provide an answer.\n\nSources:\n• data/raw/RAG_basics.md\n• data/raw/Docker_quickstart.txt\n• data/raw/LangChain_note.md\n• data/raw/FastAPI_cheatsheet.md", "similarity_score": 0.1650174856185913, "response_time_ms": 4716.3}
{"question": "How can you test the API endpoints?", "correct_answer": "By running the FastAPI server and accessing the interactive docs at /docs to try out the /upload and /ask endpoints.", "ai_answer": "To test the API endpoints, you can use the `--reload` flag with `uvicorn`, which will automatically reload the server when changes are made to the code.\n\nSources:\n• FastAPI_cheatsheet.md\n• LangChain_note.md\n• RAG_basics.md\n• Docker_quickstart.txt", "similarity_score": 0.5716270208358765, "response_time_ms": 4450.4}